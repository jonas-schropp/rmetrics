% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/binary-calc-aupr.R
\name{calc_aupr}
\alias{calc_aupr}
\alias{calc_aupr.default}
\alias{calc_aupr.table}
\alias{calc_aupr.data.frame}
\title{Calculate Area under the PR curve (AUPR).}
\usage{
calc_aupr(...)

\method{calc_aupr}{default}(tp, fp, fn, ...)

\method{calc_aupr}{table}(tbl, ...)

\method{calc_aupr}{data.frame}(data, prediction, reference, ...)
}
\arguments{
\item{...}{Additional arguments. Not used.}

\item{tp}{Numeric, True Positives (TP).}

\item{fp}{Numeric, False Positives (FP).}

\item{fn}{Numeric, Fase Negatives (FN).}

\item{tbl}{A table representing the input confusion matrix. This must always have
prediction on rows and reference on columns, otherwise most functions in
rmetrics will generate incorrect results.}

\item{data}{A data.frame containing the prediction and the reference.}

\item{prediction}{Character. The name of the variable in data that contains the predictions.}

\item{reference}{Character. The name of the variable in data that contains the reference
values.}
}
\description{
The area under the precision-recall curve (AUPRC) is a metric that is used
to evaluate the performance of a binary classification model. It is
calculated by measuring the area under the curve generated by plotting
the precision and recall of a model as functions of a probability threshold.
}
\details{
Precision and recall are two metrics that are commonly used to evaluate
the performance of a binary classification model. Precision is the ratio of
true positive predictions to the total number of positive predictions made
by the model, while recall is the ratio of true positive predictions to
the total number of actual positive samples.

The AUPRC is calculated by first generating a series of precision-recall
pairs at different probability thresholds. This is done by varying the
threshold used to classify a sample as positive or negative, and calculating
the precision and recall at each threshold. These precision-recall pairs
are then plotted on a graph, with precision on the y-axis and recall on the
x-axis. The AUPRC is then calculated by measuring the area under this curve.

The AUPRC is a useful metric because it allows for a more comprehensive
evaluation of a model's performance. Unlike the F1-score, which is a
single number that represents the harmonic mean of precision and recall,
the AUPRC takes into account the entire precision-recall curve and provides
a more complete picture of a model's performance.
}
\section{Methods (by class)}{
\itemize{
\item \code{calc_aupr(default)}: 

\item \code{calc_aupr(table)}: 

\item \code{calc_aupr(data.frame)}: 

}}
