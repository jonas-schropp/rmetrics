% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/binary-calc-aupr.R
\name{calc_aupr}
\alias{calc_aupr}
\alias{calc_aupr.default}
\alias{calc_aupr.table}
\alias{calc_aupr.data.frame}
\title{Calculate Area under the PR curve (AUPR).}
\usage{
calc_aupr(...)

\method{calc_aupr}{default}(tp, fp, fn, ...)

\method{calc_aupr}{table}(tbl, incr = FALSE, ...)

\method{calc_aupr}{data.frame}(data, prediction, reference, incr = FALSE, ...)
}
\arguments{
\item{...}{Additional arguments. Not used.}

\item{tp}{Numeric, True Positives (TP).}

\item{fp}{Numeric, False Positives (FP).}

\item{fn}{Numeric, Fase Negatives (FN).}

\item{tbl}{A table representing the input confusion matrix. This must always have
prediction on rows and reference on columns, otherwise most functions in
rmetrics will generate incorrect results.}

\item{incr}{Double. Continuity correction to add to each cell of the table. FALSE
by default, which means the raw counts will be used.}

\item{data}{A data.frame containing the prediction and the reference.}

\item{prediction}{Character. The name of the variable in data that contains the predictions.}

\item{reference}{Character. The name of the variable in data that contains the reference
values.}
}
\description{
The area under the precision-recall curve (AUPRC) is a metric that is used
to evaluate the performance of a binary classification model. It is
calculated by measuring the area under the curve generated by plotting
the precision and recall of a model as functions of a probability threshold.
}
\details{
Precision and recall are two metrics that are commonly used to evaluate
the performance of a binary classification model. Precision is the ratio of
true positive predictions to the total number of positive predictions made
by the model, while recall is the ratio of true positive predictions to
the total number of actual positive samples.

The AUPRC is calculated by first generating a series of precision-recall
pairs at different probability thresholds. This is done by varying the
threshold used to classify a sample as positive or negative, and calculating
the precision and recall at each threshold. These precision-recall pairs
are then plotted on a graph, with precision on the y-axis and recall on the
x-axis. The AUPRC is then calculated by measuring the area under this curve.
}
\section{Methods (by class)}{
\itemize{
\item \code{calc_aupr(default)}: 

\item \code{calc_aupr(table)}: 

\item \code{calc_aupr(data.frame)}: 

}}
\examples{
# Create a sample data frame with prediction and reference columns
data <- data.frame(
  prediction = c(0.9, 0.8, 0.6, 0.5, 0.3, 0.2, 0.1),
  reference = c(1, 0, 0, 1, 1, 0, 1)
)

# Calculate AUPR using the data frame method
calc_aupr(data, prediction = "prediction", reference = "reference")

# the table method
calc_aupr(table(data$prediction, data$reference))

# Calculate AUPR using the default method
tp <- 2
fp <- 1
fn <- 2
calc_aupr(tp, fp, fn)

}
\seealso{
\code{\link[=calc_precision]{calc_precision()}}

\code{\link[=calc_tpr]{calc_tpr()}}

\code{\link[=calc_auroc]{calc_auroc()}}
}
