% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/multiclass-calc-kalpha.R
\name{calc_kalpha}
\alias{calc_kalpha}
\alias{calc_kalpha.table}
\alias{calc_kalpha.data.frame}
\title{Calculate Unweighted Krippendorff's Alpha.}
\usage{
calc_kalpha(...)

\method{calc_kalpha}{table}(tbl, unbiased = TRUE, ...)

\method{calc_kalpha}{data.frame}(data, prediction, reference, ...)
}
\arguments{
\item{...}{Additional arguments. Not used.}

\item{tbl}{A table representing the input confusion matrix. This must always have
prediction on rows and reference on columns, otherwise most functions in
rmetrics will generate incorrect results.}

\item{unbiased}{TRUE/FALSE. Should unbiased overall random accuracy be used?}

\item{data}{A data.frame containing the prediction and the reference.}

\item{prediction}{Character. The name of the variable in data that contains the predictions.}

\item{reference}{Character. The name of the variable in data that contains the reference
values.}
}
\description{
Krippendorff's alpha is a statistical measure of the agreement among a set
of raters who assign ratings to a set of items. It is commonly used in the
field of content analysis, where researchers use it to assess the reliability
of their coding schemes. It can be thought of as a generalization of the
concept of inter-rater reliability, which is a measure of how well two or
more raters agree on their ratings of a set of items. Krippendorff's alpha
allows for the calculation of reliability even when there are more than two
raters, and it can also be used with ordinal or continuous data, whereas
inter-rater reliability is typically only used with binary or nominal data.
}
\details{
The alpha value ranges from 0 to 1, where 0 indicates no agreement among the
raters and 1 indicates perfect agreement. An alpha value of 0.7 or higher is
generally considered to indicate good agreement among the raters.
}
\section{Methods (by class)}{
\itemize{
\item \code{calc_kalpha(table)}: 

\item \code{calc_kalpha(data.frame)}: 

}}
