% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/binary-calc-acc.R
\name{calc_acc}
\alias{calc_acc}
\alias{calc_acc.default}
\alias{calc_acc.table}
\alias{calc_acc.data.frame}
\title{Calculate Accuracy (acc)}
\usage{
calc_acc(...)

\method{calc_acc}{default}(tp, tn, fp, fn, ci.type, ci.level, ...)

\method{calc_acc}{table}(tbl, ci.type, ci.level, ...)

\method{calc_acc}{data.frame}(data, prediction, reference, ci.type, ci.level, ...)
}
\arguments{
\item{...}{Additional arguments. Not used.}

\item{tp}{Numeric, True Positives (TP).}

\item{tn}{Numeric, True Negatives (TN).}

\item{fp}{Numeric, False Positives (FP).}

\item{fn}{Numeric, Fase Negatives (FN).}

\item{ci.type}{Either FALSE if no confidence intervals are desired or one of "agresti.coull", "agresti-coull", "ac", "asymptotic", "normal", "wald", "clopper-pearson", "cp", "exact", "jeffreys", "bayes", and "wilson". If FALSE, overwrites ci.level.}

\item{ci.level}{A number between 0 and 1 for the levels of the confidence intervals that
should be calculated.}

\item{tbl}{A table representing the input confusion matrix. This must always have
prediction on rows and reference on columns, otherwise most functions in
rmetrics will generate incorrect results.}

\item{data}{A data.frame containing the prediction and the reference.}

\item{prediction}{Character. The name of the variable in data that contains the predictions.}

\item{reference}{Character. The name of the variable in data that contains the reference
values.}
}
\description{
Accuracy refers to the degree to which a model's predictions are correct.
It is a measure of how well a model is able to make predictions that match
the observed data.
Accuracy is calculated by dividing the number of correct predictions made
by the model by the total number of predictions made.
}
\details{
Accuracy is calculated as follows:

Accuracy = (Number of correct predictions) / (Total number of predictions)

For example, if a model makes 100 predictions and 75 of them are correct,
the accuracy score would be 75/100 = 0.75. This means that the model is able
to correctly predict the outcome 75\% of the time.

Use with caution when class imbalance is present or a focus on either type
1 or type 2 errors is necessary.
}
\section{Methods (by class)}{
\itemize{
\item \code{calc_acc(default)}: 

\item \code{calc_acc(table)}: 

\item \code{calc_acc(data.frame)}: 

}}
